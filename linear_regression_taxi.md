# 芝加哥出租车费用预测项目总结

## 1. 项目概述

本项目旨在构建一个能够根据行程信息预测芝加哥出租车费用的机器学习模型。项目从一个简单的单特征线性回归模型开始，通过一个完整的、迭代式的分析与优化周期，最终产出了一个健壮、智能、且可在现实世界中部署的“智能单特征模型”。

整个项目的核心历程，是从一个单纯的“模型训练问题”，逐步深化为一个“数据质量问题”，并最终升华为一个“模型能力与特征工程问题”。我们不仅得到了一个可用的模型，更重要的是，我们建立了一套完整、专业、且可复现的数据处理与模型评估流程，并对业务场景的复杂性获得了深刻的洞察。

## 2. 核心思想演进

我们的策略经历了三个关键阶段的演进：

1.  **初步建模阶段**: 使用原始数据直接训练，发现模型性能极差，预测误差巨大。这让我们意识到，数据质量是模型性能的瓶颈。
2.  **以数据为中心的清洗阶段**: 这是我们投入精力最多的阶段。我们没有盲目地调整模型，而是专注于理解和清洗数据。通过一系列迭代（从简单的百分位数法，到关系审查，再到复合规则，最终到分组定制规则），我们成功地定义并实现了一套能够剔除不合逻辑数据、同时又保留合理边缘案例的智能清洗算法。
3.  **智能化特征工程阶段**: 在拥有了高质量的数据集后，我们诊断出简单线性模型的内在局限性。为了解决“预测时无法获取未来信息（如真实行驶时间）”这一核心业务痛点，我们通过特征工程，从历史数据中提炼出“预估时间”这一关键特征，最终构建出一个兼具较高精度和现实可用性的“智能模型”。

## 3. 各脚本详细解析

以下是项目中各个核心脚本的作用、流程与所使用算法的详细解析。

---

### **`src/split_data.py` (核心数据处理脚本)**

*   **核心作用**: 这是整个项目的**数据处理中枢**和**单一事实来源 (Single Source of Truth)**。它的唯一职责是，接收最原始的数据文件 (`chicago_taxi_google.csv`)，执行我们最终确定的、最智能的清洗逻辑，然后生成供所有下游模型使用的、统一的、干净的训练集、验证集和测试集。

*   **处理流程**:
    1.  **加载**：读取原始的 `chicago_taxi_google.csv` 文件。
    2.  **初步过滤**: 移除 `FARE`, `TRIP_MILES`, `TRIP_MINUTES` 中任何小于或等于0的、物理上不可能的行程。
    3.  **智能分组**: 将数据**划分为两个独立群体**进行区别对待，这是整个清洗逻辑的核心：
        *   **群体A：短途行程** (`TRIP_MILES <= 1.0`)
        *   **群体B：非短途行程** (`TRIP_MILES > 1.0`)
    4.  **定制化规则计算**:
        *   对于**群体A**，单独计算其 `FARE` 的99.9百分位数，以此作为该群体的“费用硬上限”。
        *   对于**群体B**，单独计算其 `price_per_mile` 的Q1, Q3和IQR，以此作为该群体的“单位票价上限”。
    5.  **执行清洗**: 根据为每个群体量身定制的规则，结合对所有行程都适用的“速度异常”和“单位票价过低”规则，识别并**移除**所有异常记录。
    6.  **数据分割**: 对最终得到的、干净的数据集，使用 `train_test_split` 函数执行标准的80/10/10分割。
    7.  **覆盖保存**: 将三个干净的数据集，直接覆盖保存为 `chicago_taxi_train.csv`, `chicago_taxi_validation.csv`, `chicago_taxi_test.csv`。

*   **关键参数与选择原因**:
    *   `short_trip_mile_threshold = 1.0`: 将1英里作为短途和非短途的分割线，这是一个基于常识的判断，因为起步价通常覆盖了最初一小段距离。
    *   `short_trip_fare_quantile = 0.999`: 对短途行程的费用，我们使用99.9百分位数作为上限。这是一种数据驱动的方法，旨在移除那些极端罕见的“天价”短途行程，同时保留所有在统计上合理的费用。
    *   `IQR Multiplier = 1.5`: 在计算非短途行程的单位票价上限时，我们使用了 `Q3 + 1.5 * IQR` 这个标准统计学公式。`1.5` 是一个在统计学中被广泛接受的、用于识别温和异常值的标准系数。
    *   `average_speed_bounds = (1.0, 80.0)`: 这个速度范围是基于物理常识设定的。低于1mph比步行还慢，高于80mph在市区内几乎不可能，都可以被认为是数据记录错误。
    *   `random_state = 42`: 在分割数据时固定随机种子，是为了保证实验的**可复现性**。无论我们运行脚本多少次，都能得到完全相同的训练、验证和测试集，这对于调试和公平地比较模型性能至关重要。

---

### **`src/train_*.py` (模型训练脚本)**

这是一个脚本家族，包含了我们项目迭代过程中的所有模型。

*   **核心作用**: 负责定义、训练和保存一个特定版本的Keras模型。
*   **包含的脚本**:
    *   `train_single_feature.py`: 训练一个最基础的、只使用 `TRIP_MILES` 的**基准模型**。
    *   `train_multi_feature.py`: 训练一个使用 `TRIP_MILES` 和 **真实** `TRIP_MINUTES` 的**诊断模型**，其主要价值是证明“时间”因素的重要性。
    *   `train_smart_model.py`: 训练我们最终的、使用 `TRIP_MILES` 和 **预估** `ESTIMATED_MINUTES` 的**生产模型**。

*   **处理流程**:
    1.  定义实验名称、输入特征、学习率等超参数。
    2.  **对于 `train_smart_model`**: 首先从 `artifacts` 目录加载 `time_distance_lookup.json` 配置文件。
    3.  加载干净的 `chicago_taxi_train.csv` 数据。
    4.  **对于 `train_smart_model`**: 执行**特征工程**，利用加载的规则，为训练数据动态创建 `ESTIMATED_MINUTES` 新特征。
    5.  定义并编译一个Keras线性回归模型。
    6.  在处理好的训练数据上执行模型训练。
    7.  将训练好的模型 (`.keras`) 和其配置 (`.pkl`) 保存到 `artifacts` 目录中。

*   **关键参数与选择原因**:
    *   `learning_rate=0.001`, `number_epochs=20`, `batch_size=50`: 这些是针对此类简单的线性回归任务，非常标准和稳健的初始超参数组合。它们能在保证模型充分收敛的同时，有效防止训练过程中的梯度爆炸或过拟合问题。

---

### **`src/test.py` (统一模型评估脚本)**

*   **核心作用**: 提供一个标准化的最终评估平台，用于在干净的测试集上，公平地衡量任何一个已训练模型的真实性能。

*   **处理流程**:
    1.  通过命令行参数，接收一个要评估的 `experiment_name` (如 `smart_model`)。
    2.  加载 `artifacts` 目录中对应的模型 (`.keras`) 和配置 (`.pkl`)。
    3.  加载干净的 `chicago_taxi_test.csv` 数据。
    4.  **核心逻辑**: 如果评估的是 `smart_model`，脚本会**执行与训练时完全相同的特征工程步骤**：加载 `time_distance_lookup.json`，并为**测试集**动态创建 `ESTIMATED_MINUTES` 特征。这保证了训练和测试的一致性。
    5.  使用加载的模型，对处理好的测试集生成全部预测。
    6.  计算并打印标准的**模型指标** (如 `RMSE`) 和我们自定义的**业务指标** (如 `Errors > $5.00` 的比例)。
    7.  根据用户的可选参数，打印最差和最佳的预测样本，以供深度分析。

*   **关键参数与选择原因**:
    *   `large_error_threshold = 5.0`: 将$5.00作为判断“严重错误”的界限，这是一个基于业务理解的自定义指标。它比单纯的RMSE更能直观地反映出模型在实际使用中，犯下不可接受错误的频率。

---

### **`src/analyze_time_distance.py` (分析与配置生成脚本)**

*   **核心作用**: 这是我们“智能模型”的**大脑构建器**。它是一个一次性的分析脚本，负责从历史数据中提炼知识，并将其转化为一个可供其他脚本使用的配置文件。

*   **处理流程**:
    1.  加载干净的 `chicago_taxi_train.csv` 数据。
    2.  将 `TRIP_MILES` 按1英里为间隔进行分箱。
    3.  使用 `groupby()` 计算出每个距离区间的平均行程时间 `average_minutes`。
    4.  将这个计算出的“速查表”保存为一个结构清晰的 `time_distance_lookup.json` 文件，存放在 `artifacts` 目录中。

*   **使用到的关键算法/方法**:
    *   **Pandas `groupby()` & `cut()`**: 用于执行核心的分箱与聚合分析。
    *   **JSON**: 用于将分析结果序列化并保存为可复用的配置文件。

---

## 4. 项目总结与展望

本项目成功地完成了一个从数据探索到模型部署的全周期。我们最重要的成果，不仅仅是一个预测模型，而是一套**以数据为中心、可持续迭代的、健壮的机器学习工作流**。

我们清晰地证明了，对于此类问题，**智能的数据清洗和巧妙的特征工程，其重要性远超于模型结构本身**。我们最终的 `smart_model`，虽然本质上仍是简单的线性回归，但因为它吸收了我们对业务和数据的深刻洞察，其在现实中的可用性和价值，已经远超于那些理论上更强大但无法直接部署的模型。

### **下一步展望**

我们已经将线性模型的能力挖掘到了极限，下一步的优化方向将不再是数据清洗，而是提升模型自身的能力和特征的维度。

#### **1. 增强单特征模型 (更好的线性模型)**

即使我们坚持使用线性回归，也依然有提升空间。我们可以通过更高级的特征工程，让它学习非线性关系：
*   **多项式特征 (Polynomial Features)**: 创建 `TRIP_MILES` 的平方项 (`miles^2`) 或立方项作为新特征。这能让模型学习到一条曲线而不是直线，例如“随着距离增加，每英里单价逐渐降低”的规律。
*   **交互特征 (Interaction Features)**: 创建 `TRIP_MILES` 和 `ESTIMATED_MINUTES` 的乘积项 (`miles * minutes`) 作为新特征。这能让模型学习到“时间和距离的共同作用会产生额外影响”的复杂逻辑，例如“在长途行程中，时间的价值更高”。

#### **2. 使用更强大的模型**

这是最直接、最高效的下一步。我们可以无缝地将高质量的数据集，喂给那些本身就能学习复杂非线性关系的现代模型：
*   **梯度提升树 (如 XGBoost, LightGBM)**: 这是处理此类表格数据的“业界标杆”。它们能自动从数据中学习到复杂的决策规则（例如“如果距离小于1英里，就启用一套完全不同的收费规则”），无需我们手动进行复杂的特征工程。
*   **深度神经网络 (Deep Neural Networks)**: 我们可以构建一个包含多个隐藏层和非线性激活函数（如 ReLU）的更深层的网络，赋予模型更强大的函数拟合能力。

#### **3. 引入新维度的特征**

*   **类别特征编码 (Categorical Feature Encoding)**: 利用 `PICKUP_COMMUNITY_AREA` 等地理信息。通过**One-Hot Encoding**等技术，将“区域76（机场）”这样的类别信息，转化为模型可以理解的数值特征，让模型能明确地学习到不同区域（如机场、市中心）的不同计费模式。

### **需要警惕的陷阱：目标泄漏 (Target Leakage)**

在未来的特征工程中，必须严格遵守一个原则：**不能使用任何在真实预测时刻无法获得的信息作为特征**。在本项目中，以下字段是**绝对禁止**在训练中使用的：
*   `TIPS` (小费)
*   `TOLLS` (路费)
*   `EXTRAS` (附加费)
*   `TRIP_TOTAL` (总费用)

**原因**: 这些都是在行程**结束后**才能产生的“结果性”数据。如果在训练中使用了它们，模型会学会“作弊”（例如 `FARE ≈ TRIP_TOTAL - TIPS`），在测试集上产生虚假的高分，但在真实预测场景中会因为无法获得这些输入而完全失效。

## 5. 脚本执行命令指南

为了方便回顾和复现，以下是本项目核心脚本的推荐执行顺序和命令。请在项目根目录 (`linear_regression_taxi`) 下运行。

```bash
# (可选) 步骤 0: 运行分析脚本，检查数据异常情况
# 这个脚本只读不改，用于探索性分析。
python -m src.find_outliers

# 步骤 1: 生成“距离-时间”速查表 (我们智能模型的大脑)
python -m src.analyze_time_distance

# 步骤 2: 执行数据清洗与分割
# 这会读取原始数据，应用我们最终的清洗算法，并生成三个干净的CSV文件。
python -m src.split_data

# 步骤 3: 训练所有模型
# 依次训练我们的基准模型、诊断模型和最终的智能模型。
python -m src.train_single_feature
python -m src.train_multi_feature
python -m src.train_smart_model

# 步骤 4: 评估所有模型
# 在干净的测试集上，对每个模型进行最终的性能评估和比较。
python -m src.test single_feature
python -m src.test multi_feature
python -m src.test smart_model

# (可选) 查看特定数量的最差/最佳预测样本
python -m src.test smart_model --show-predictions 15
```
